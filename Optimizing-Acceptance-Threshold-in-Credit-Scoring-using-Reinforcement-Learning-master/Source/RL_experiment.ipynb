{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing Acceptance Threshold in Credit Scoring using Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### External libraries used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                         # to work with vectors and matrices\n",
    "import pandas as pd                        # to work with data \n",
    "import datetime as dt                      # to work with dates\n",
    "import joblib                              # to store python objects as files\n",
    "import matplotlib.pyplot as plt            # to vizualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom classes used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulation environment of the micro-loan business\n",
    "from simulation3_2_2 import SimulationEnv\n",
    "# the reinforcement learning agent\n",
    "from agent import Agent\n",
    "# models used by the RL agent to interact with and learn from environment \n",
    "from model import FeatureTransformer, Model, EnvironmentModel\n",
    "# policy that the RL agent follows when interacting with the environment\n",
    "from policy import Policy\n",
    "# manager to hide a couple hundreds rows of code and to keep the presentation neat\n",
    "from manager import Manager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the environment and initializing the RL agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# micro-loan business simulation environment instance\n",
    "env = SimulationEnv()\n",
    "# feature transformer instance to convert numerous outputs of environment into simple numeric variables understood by the RL agent\n",
    "ft = FeatureTransformer(env)\n",
    "# value function model instance - the brain of the RL agent. Approximates value of each action in every state of environment\n",
    "lr = 0.0001                               # learning rate defines how adaptive the value function is to new input\n",
    "model = Model(env, ft, lr)\n",
    "# environment model instance - the planning center of the agent. Predicts future environment states based on the current one\n",
    "env_model = EnvironmentModel(env, lr)\n",
    "# policy instance - includes different kinds of behaviors the agent can use to interact with the environment\n",
    "policy = Policy(env)\n",
    "# RL agent instance - the guy that uses all of the above in order to optimize whatever you need\n",
    "eps = 1                                   # exploration rate defines how much randomness to use to explore the environment\n",
    "gamma = 0.95                              # discounting rate defines how quick the agent forgets his previous experience\n",
    "agent = Agent(env, model, env_model, policy, eps, gamma, gamma)\n",
    "# manager instance - a class to manage the experiment\n",
    "manager = Manager(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the RL experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define train and test episode numbers\n",
    "train_episodes = 100                        # number of train episodes, where agent learns the environment and value function\n",
    "test_episodes = 5                           # number of test episodes in a row to evaluate the current agent\n",
    "test_frequency = 5                          # frequency of testing to track the progress of the agent\n",
    "distorted_episodes = 100                    # number of test episodes in a distorted environment to evaluate ability to adapt\n",
    "\n",
    "# define variables to store the experiment history\n",
    "name = 'baseline'                           # name of experiment\n",
    "bookkeeping_directory = ''                  # directory to store history\n",
    "bookkeeping_frequency = 1                   # frequency of storing\n",
    "\n",
    "# initialize the experiment\n",
    "manager.initExperiment(train_episodes = train_episodes, test_episodes = test_episodes, test_frequency = test_frequency, experiment_name = name, bookkeeping_directory = bookkeeping_directory, bookkeeping_frequency = bookkeeping_frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run one episode with initial agent having no knowledge of environment\n",
    "test_episode_progress = manager.runTestEpisode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# visualize episode progress\n",
    "manager.plotEpisode(test_episode_progress)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First episode of learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run one episode with initial agent starting to learn from its interaction with the environment\n",
    "episode_progress = manager.runTrainEpisode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# visualize episode progress\n",
    "manager.plotEpisode(episode_progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# visualize value function\n",
    "manager.plot_q_values(episode = 0)\n",
    "manager.progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 100 more episodes of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# run 100 episodes of training\n",
    "weekly_progress, progress = manager.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# visualize training progress\n",
    "manager.plotRun(weekly_progress, progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "# visualize value function\n",
    "manager.plot_q_values(episode = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducing dynamics into the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# add distortions to the simulated environment: decrease in the average predicted probability to repay\n",
    "distortions = {'e': 1, \n",
    "               'news_positives_score_bias': -2,\n",
    "               'repeats_positives_score_bias': -1,\n",
    "               'news_negatives_score_bias': 2,\n",
    "               'repeats_negatives_score_bias': 1,\n",
    "               'news_default_rate_bias': 0,\n",
    "               'repeats_default_rate_bias': 0, \n",
    "               'late_payment_rate_bias': 0, \n",
    "               'ar_effect': 0}\n",
    "env = SimulationEnv(distortions = distortions)\n",
    "\n",
    "# adjust learning and discount rates enabling the agent to adapt more efficiently\n",
    "lr = 0.001\n",
    "gamma = 0.5\n",
    "eps = 0.5\n",
    "model = joblib.load('/bookkeeping/' + name + '/episode_' + str(train_episodes) + '/model.pkl')\n",
    "model.set_learning_rate(lr)\n",
    "\n",
    "# pass adjustments to the current agent\n",
    "agent.env = env\n",
    "agent.model = model\n",
    "agent.gamma1 = agent.gamma2 = gamma\n",
    "agent.eps = eps\n",
    "\n",
    "# pass adjusted agent to the manager\n",
    "manager.agent = agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First distorted episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# run one episode of distorted environment\n",
    "distorted_episode_progress = manager.runDistortedEpisode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# visualize episode progress\n",
    "manager.plotEpisode(distorted_episode_progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Visualize value function\n",
    "manager.plot_q_values(episode = 'distorted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 100 more episodes of adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# run 100 distorted episodes\n",
    "distorted_weekly_progress, progress = manager.simulateDistortedEpisodes(distortions, lr, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# visualize RL agent's performance\n",
    "manager.plotDistortedEpisodes(distorted_weekly_progress, progress)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
